{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e22c699e",
   "metadata": {},
   "source": [
    "# Desired Performance\n",
    "\n",
    "Given our near-perfect results from project 1 (with our hyper-tuned KNeighborsClassifier and hyper-tuned DecisionTreeClassifier), we would expect similar or better results for our ensemble and deep learning methods.\n",
    "\n",
    "Ensemble Learning: \n",
    "\n",
    "- We desire these to match as closely as possible to our previous results, but expect they will likely fall short. Combination methods like VotingClassifier will likely be hurt by worse models, and boosting methods like XGBoost may overfit despite their regularization.\n",
    "\n",
    "Deep Learning:\n",
    "\n",
    "- For neural networks, we expect the greatest potential but also the most uncertainty. With 2 million samples, deep learning should theoretically excel, but tabular data often remains challenging for neural networks compared to tree-based methods. We anticipate that a carefully architected neural network with appropriate regularization could match our previous performance, though reaching our near-perfect baseline may require extensive tuning, which we may not have time for.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
